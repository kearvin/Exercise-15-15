{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Exercise 15.15 — Linear Regression with the Diabetes Dataset (scikit-learn)\n\n**Goal:** Recreate the same overall workflow from the Chapter 15.5 case study, but using the **Diabetes** dataset.\n\n**What this notebook does:**\n1. Load the Diabetes dataset from `sklearn.datasets`\n2. Explore the data (shape, feature names, basic stats)\n3. Visualize relationships and the target distribution\n4. Build a baseline Linear Regression model\n5. Evaluate with train/test split + metrics\n6. Try a few model-selection ideas (Ridge/Lasso, Polynomial features) and compare results\n\n> **Tip:** Rewrite the markdown explanations in your own words before submitting.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\npd.set_option(\"display.max_columns\", 50)\npd.set_option(\"display.width\", 120)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1) Load the Diabetes dataset\nThe Diabetes dataset has 10 baseline variables (features) and a quantitative target (disease progression one year after baseline).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "diabetes = load_diabetes(as_frame=True)\n\nX = diabetes.data      # DataFrame\ny = diabetes.target    # Series\n\nprint(\"X shape:\", X.shape)\nprint(\"y shape:\", y.shape)\nX.head()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Feature names\ndiabetes.feature_names\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2) Quick data checks\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "X.info()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Missing values (should be none)\nX.isna().sum()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Basic stats for features and target\ndisplay(X.describe())\ndisplay(y.describe())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3) Visualizations\nWe'll look at:\n- Histogram of the target\n- A few scatter plots of features vs target\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "plt.figure(figsize=(7,4))\nplt.hist(y, bins=30)\nplt.title(\"Target Distribution (Diabetes progression)\")\nplt.xlabel(\"Target\")\nplt.ylabel(\"Count\")\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Pick a few features to visualize vs target\nfeatures_to_plot = [\"bmi\", \"bp\", \"s5\", \"s1\"]\n\nfor f in features_to_plot:\n    plt.figure(figsize=(6,4))\n    plt.scatter(X[f], y, s=10)\n    plt.title(f\"{f} vs Target\")\n    plt.xlabel(f)\n    plt.ylabel(\"Target\")\n    plt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4) Train/Test split\nWe'll hold out a test set to evaluate generalization.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5) Baseline model: Linear Regression\nWe'll train a basic Linear Regression model and evaluate using:\n- MAE\n- MSE / RMSE\n- R²\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "lin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\ny_pred = lin_reg.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Linear Regression Performance (Test Set)\")\nprint(\"MAE :\", round(mae, 3))\nprint(\"MSE :\", round(mse, 3))\nprint(\"RMSE:\", round(rmse, 3))\nprint(\"R^2 :\", round(r2, 3))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Predicted vs Actual plot\nIf the model is good, points should fall close to a diagonal line.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "plt.figure(figsize=(6,6))\nplt.scatter(y_test, y_pred, s=12)\nplt.title(\"Predicted vs Actual (Linear Regression)\")\nplt.xlabel(\"Actual y\")\nplt.ylabel(\"Predicted y\")\n\nmin_y = min(y_test.min(), y_pred.min())\nmax_y = max(y_test.max(), y_pred.max())\nplt.plot([min_y, max_y], [min_y, max_y])\nplt.show()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6) Model selection ideas (like the case study style)\n\n### A) Ridge Regression (L2 regularization)\nWe use a Pipeline with StandardScaler because regularization is scale-sensitive.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "ridge_model = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"ridge\", Ridge(alpha=1.0, random_state=42))\n])\n\nridge_model.fit(X_train, y_train)\nridge_pred = ridge_model.predict(X_test)\n\nridge_mse = mean_squared_error(y_test, ridge_pred)\nridge_rmse = np.sqrt(ridge_mse)\nridge_mae = mean_absolute_error(y_test, ridge_pred)\nridge_r2 = r2_score(y_test, ridge_pred)\n\nprint(\"Ridge Regression Performance (Test Set)\")\nprint(\"MAE :\", round(ridge_mae, 3))\nprint(\"RMSE:\", round(ridge_rmse, 3))\nprint(\"R^2 :\", round(ridge_r2, 3))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### B) Lasso Regression (L1 regularization)\nLasso can shrink some coefficients to zero (feature selection-ish behavior).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "lasso_model = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"lasso\", Lasso(alpha=0.05, max_iter=10000, random_state=42))\n])\n\nlasso_model.fit(X_train, y_train)\nlasso_pred = lasso_model.predict(X_test)\n\nlasso_mse = mean_squared_error(y_test, lasso_pred)\nlasso_rmse = np.sqrt(lasso_mse)\nlasso_mae = mean_absolute_error(y_test, lasso_pred)\nlasso_r2 = r2_score(y_test, lasso_pred)\n\nprint(\"Lasso Regression Performance (Test Set)\")\nprint(\"MAE :\", round(lasso_mae, 3))\nprint(\"RMSE:\", round(lasso_rmse, 3))\nprint(\"R^2 :\", round(lasso_r2, 3))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### C) Polynomial features + Linear Regression\nThis can capture non-linear relationships, but it can also overfit.\nWe'll try degree=2 as a basic experiment.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "poly2_model = Pipeline([\n    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n    (\"scaler\", StandardScaler()),\n    (\"linreg\", LinearRegression())\n])\n\npoly2_model.fit(X_train, y_train)\npoly2_pred = poly2_model.predict(X_test)\n\npoly2_mse = mean_squared_error(y_test, poly2_pred)\npoly2_rmse = np.sqrt(poly2_mse)\npoly2_mae = mean_absolute_error(y_test, poly2_pred)\npoly2_r2 = r2_score(y_test, poly2_pred)\n\nprint(\"Polynomial (degree=2) + Linear Regression Performance (Test Set)\")\nprint(\"MAE :\", round(poly2_mae, 3))\nprint(\"RMSE:\", round(poly2_rmse, 3))\nprint(\"R^2 :\", round(poly2_r2, 3))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7) Compare models (summary table)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "results = pd.DataFrame({\n    \"Model\": [\"Linear Regression\", \"Ridge (alpha=1.0)\", \"Lasso (alpha=0.05)\", \"Poly2 + Linear\"],\n    \"MAE\":  [mae, ridge_mae, lasso_mae, poly2_mae],\n    \"RMSE\": [rmse, ridge_rmse, lasso_rmse, poly2_rmse],\n    \"R2\":   [r2, ridge_r2, lasso_r2, poly2_r2]\n}).sort_values(\"RMSE\")\n\nresults\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8) Cross-validation (optional but good practice)\nTo avoid relying on a single train/test split, we can compare models using cross-validation.\nWe’ll use **R²** as the scoring metric here.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "def cv_r2(model, X, y, folds=5):\n    scores = cross_val_score(model, X, y, cv=folds, scoring=\"r2\")\n    return scores.mean(), scores.std()\n\nmodels = {\n    \"Linear Regression\": LinearRegression(),\n    \"Ridge (alpha=1.0)\": ridge_model,\n    \"Lasso (alpha=0.05)\": lasso_model,\n    \"Poly2 + Linear\": poly2_model\n}\n\ncv_rows = []\nfor name, model in models.items():\n    mean_r2, std_r2 = cv_r2(model, X, y, folds=5)\n    cv_rows.append((name, mean_r2, std_r2))\n\ncv_df = pd.DataFrame(cv_rows, columns=[\"Model\", \"CV Mean R2\", \"CV Std R2\"]).sort_values(\"CV Mean R2\", ascending=False)\ncv_df\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9) Conclusion (rewrite in your own words)\n\nIn this notebook, I loaded the Diabetes dataset from scikit-learn, explored the features and target,\nand built a baseline Linear Regression model. I evaluated performance using standard regression metrics and a predicted vs actual plot.\nThen I tested model-selection ideas (Ridge, Lasso, and polynomial features) to see if they improved accuracy.\nFinally, I used cross-validation to compare models more reliably across multiple splits.\n"
    }
  ],
  "metadata": {
    "colab": {
      "name": "Ex_15_15_Diabetes_Linear_Regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}